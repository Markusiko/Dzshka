{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/markstolyarov/hw2-stolyarov?scriptVersionId=91859068\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<center><img src=\"https://github.com/hse-ds/iad-applied-ds/blob/master/2021/hw/hw1/img/logo_hse.png?raw=1\" width=\"1000\"></center>\n\n<h1><center>Прикладные задачи анализа данных</center></h1>\n<h2><center>Домашнее задание 2: deep learning для обработки звука</center></h2>","metadata":{"id":"sdwwxjAQFXfw"}},{"cell_type":"markdown","source":"# Введение\n\nВ этом задании Вы поработаете и разберетесь в деталях с форматами представления аудиоданных в задачах глубинного обучения, а так же напишете несколько моделей для классификации аудиозаписей.\n\nВ процессе выполнения Вы познакомитесь:\n* С алгоритмом построения Мел-спектрограммы\n* Рекуррентными и сверточными классификаторами аудиоданных\n* Алгоритмом аугментации аудиоданных SpecAugment","metadata":{"id":"7YFkvK6vFXf9"}},{"cell_type":"code","source":"!pip install torch==1.8.0 torchaudio==0.8.0 numpy==1.20.0","metadata":{"id":"DPQ9427wjLeX","outputId":"f9ca3a03-5662-4945-90e9-e642e969ad6a","execution":{"iopub.status.busy":"2022-04-01T15:12:11.978259Z","iopub.execute_input":"2022-04-01T15:12:11.978702Z","iopub.status.idle":"2022-04-01T15:12:20.055578Z","shell.execute_reply.started":"2022-04-01T15:12:11.978619Z","shell.execute_reply":"2022-04-01T15:12:20.054605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport torch\nimport torchaudio\nfrom IPython import display\nfrom IPython.display import clear_output\nfrom sklearn.metrics import confusion_matrix\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\n\n%matplotlib inline\n\nassert torch.__version__.startswith(\"1.8.0\")\nassert torchaudio.__version__ == \"0.8.0\"\n\ndevice=\"cuda:0\"","metadata":{"id":"37r09zfzFXgE","execution":{"iopub.status.busy":"2022-04-01T15:12:20.061031Z","iopub.execute_input":"2022-04-01T15:12:20.061461Z","iopub.status.idle":"2022-04-01T15:12:20.915922Z","shell.execute_reply.started":"2022-04-01T15:12:20.061424Z","shell.execute_reply":"2022-04-01T15:12:20.915043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Классификация аудиозаписей.","metadata":{"id":"Hm0T-WW8FXgF"}},{"cell_type":"markdown","source":"В этом домашнем задании Вам предстоит классифицировать аудиозаписи из датасета [UrbanSound8K](https://urbansounddataset.weebly.com/urbansound8k.html).\n\nДанный датасет состоит из 8732 записей, разбитых на train/val/test датасеты. \n\n![image](https://paperswithcode.com/media/datasets/UrbanSound8K-0000003722-02faef06.jpg)\n\nКаждая аудиозапись содержит запись шума города и принадлежит одному из 10 классов: \n\n`[air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, siren, street_music]`\n","metadata":{"id":"jak4m4AzFXgF"}},{"cell_type":"markdown","source":"## Задание 1 (1 балл). Знакомство с данными.\n\n1. Скачайте датасет из [Google Drive](https://drive.google.com/file/d/12emmtpodmo1783e6VOOEjV20zAKl5dZR/view?usp=sharing) c и распакуйте в папку `./data`.\n\n2. Напишите `AudioDataset` класс, который будет принимать путь к файлам `train_part.csv` и `val_part.csv` и возращать тройки объектов `(x, y, len)`, где `x` - аудиозапись, `y` - класс аудиозаписи, `len` - длина аудиозаписи. Аудиозаписи **не должны постоянно храниться в RAM**, подгрузку _wav_ файлов надо сделать при запросе через `__getitem__` метод. Кроме того, надо сделать паддинг аудиозаписи - если она короче чем `pad_size` параметр, надо дополнять ее нулями.\n\n3. С помощью функции `display.Audio` проиграйте в ноутбуке пару аудиозаписей.\n","metadata":{"id":"CL4IbYsbH_EW"}},{"cell_type":"code","source":"# скачаем и распакуем данные\n!rm -r ./data\n!mkdir ./data/ \n!pip install gdown\n!cd ./data && gdown 'https://drive.google.com/uc?id=12emmtpodmo1783e6VOOEjV20zAKl5dZR&confirm=t' && unzip HW2_dataset.zip && rm HW2_dataset.zip","metadata":{"id":"V4tREl4VFXgG","outputId":"72bdbb9e-fb87-4803-ff79-1a0b466b2054","execution":{"iopub.status.busy":"2022-04-01T15:12:20.918329Z","iopub.execute_input":"2022-04-01T15:12:20.918837Z","iopub.status.idle":"2022-04-01T15:12:31.110297Z","shell.execute_reply.started":"2022-04-01T15:12:20.918799Z","shell.execute_reply":"2022-04-01T15:12:31.109441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"У меня почему-то все работало с загрузкой, но в последний день сломалось, поэтому я подгружал локально с кэггла","metadata":{}},{"cell_type":"code","source":"# классы данных\nclasses = [\n    \"air_conditioner\", \n    \"car_horn\", \n    \"children_playing\", \n    \"dog_bark\",\n    \"drilling\", \n    \"engine_idling\", \n    \"gun_shot\", \n    \"jackhammer\", \n    \"siren\", \n    \"street_music\"\n]","metadata":{"id":"kCsOPjrAgs62","execution":{"iopub.status.busy":"2022-04-01T15:12:31.113299Z","iopub.execute_input":"2022-04-01T15:12:31.114051Z","iopub.status.idle":"2022-04-01T15:12:31.119356Z","shell.execute_reply.started":"2022-04-01T15:12:31.114002Z","shell.execute_reply":"2022-04-01T15:12:31.11849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(\n        self, \n        path_to_csv: str, \n        path_to_folder: str, \n        pad_size: int = 384000,\n        sr: int = 44100\n    ):\n        self.csv: pd.DataFrame = pd.read_csv(path_to_csv)[[\"ID\", \"Class\"]]\n        self.path_to_folder = path_to_folder\n        self.pad_size = pad_size\n\n        self.sr = sr\n\n        self.class_to_idx = {classes[i]: i for i in range(10)}\n\n    def __getitem__(self, index: int):\n        \n        # за основу брал семинарский код\n        output = self.csv.iloc[index]\n\n        if len(output) == 2:\n            ID, Class = output\n            y = self.class_to_idx[Class]\n        else:\n            ID = output\n            y = torch.LongTensor([-1])\n\n        path = os.path.join(self.path_to_folder, str(ID) + '.wav')\n        wav, sr = torchaudio.load(path)\n        len_audio = wav.size()[1]\n        \n        if len_audio < self.pad_size:\n            to_pad = torch.zeros(self.pad_size, dtype=torch.float32)\n            to_pad[:len_audio] = wav\n            wav = to_pad\n            \n        if sr != self.sr:\n            resampler = torchaudio.transforms.Resample(sr, self.sr)\n            wav = resampler(wav)\n        \n        wav = wav.squeeze()\n        instance = {\n            'x': wav,\n            'y': y,\n            'len': len_audio\n        }\n\n        return instance\n        \n    def __len__(self):\n        return self.csv.shape[0]","metadata":{"id":"lX3plyGFiUFV","execution":{"iopub.status.busy":"2022-04-01T15:12:31.120933Z","iopub.execute_input":"2022-04-01T15:12:31.121468Z","iopub.status.idle":"2022-04-01T15:12:31.135093Z","shell.execute_reply.started":"2022-04-01T15:12:31.121431Z","shell.execute_reply":"2022-04-01T15:12:31.134256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# создадим датасеты\ntrain_dataset = AudioDataset(\"../input/dataaa/urbansound8k/train_part.csv\", \"../input/dataaa/urbansound8k/data\")\nval_dataset = AudioDataset(\"../input/dataaa/urbansound8k/val_part.csv\", \"../input/dataaa/urbansound8k/data\")","metadata":{"id":"zleXZ2vBLy6l","execution":{"iopub.status.busy":"2022-04-01T15:12:31.137065Z","iopub.execute_input":"2022-04-01T15:12:31.137683Z","iopub.status.idle":"2022-04-01T15:12:31.161093Z","shell.execute_reply.started":"2022-04-01T15:12:31.137653Z","shell.execute_reply":"2022-04-01T15:12:31.160465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# проверим размеры датасетов\nassert len(train_dataset) == 4500\nassert len(val_dataset) == 935","metadata":{"id":"majqJhy0ix0C","execution":{"iopub.status.busy":"2022-04-01T15:12:31.162157Z","iopub.execute_input":"2022-04-01T15:12:31.162627Z","iopub.status.idle":"2022-04-01T15:12:31.166981Z","shell.execute_reply.started":"2022-04-01T15:12:31.162594Z","shell.execute_reply":"2022-04-01T15:12:31.166189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# проверим возращаемые значения __getitem__\nitem = train_dataset.__getitem__(0)\n\nassert item[\"x\"].shape == (384000, )\nassert item[\"y\"] == 0\nassert item[\"len\"] == 176400","metadata":{"id":"phsH9zRJjIT1","execution":{"iopub.status.busy":"2022-04-01T15:12:31.168327Z","iopub.execute_input":"2022-04-01T15:12:31.168909Z","iopub.status.idle":"2022-04-01T15:12:31.180957Z","shell.execute_reply.started":"2022-04-01T15:12:31.168821Z","shell.execute_reply":"2022-04-01T15:12:31.180083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# нарисуем и проиграем аудиозаписить\nitem = train_dataset.__getitem__(2014)\nplt.figure(figsize=(16, 8))\nplt.plot(item[\"x\"])\n\ndisplay.Audio(item[\"x\"], rate=train_dataset.sr)","metadata":{"id":"aIED1KdioqZ8","outputId":"eb6685d6-f5de-465f-c073-5b70a9f05fdf","execution":{"iopub.status.busy":"2022-04-01T15:12:31.182411Z","iopub.execute_input":"2022-04-01T15:12:31.183006Z","iopub.status.idle":"2022-04-01T15:12:34.223666Z","shell.execute_reply.started":"2022-04-01T15:12:31.182971Z","shell.execute_reply":"2022-04-01T15:12:34.222954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# нарисуем и проиграем аудиозаписить\nitem = train_dataset.__getitem__(11)\nplt.figure(figsize=(16, 8))\nplt.plot(item[\"x\"])\n\ndisplay.Audio(item[\"x\"], rate=train_dataset.sr)","metadata":{"id":"E9dH-ErCpe0K","outputId":"a4e3334f-76dd-4bbe-b5fe-ed0ab727c3bf","execution":{"iopub.status.busy":"2022-04-01T15:12:34.225011Z","iopub.execute_input":"2022-04-01T15:12:34.225436Z","iopub.status.idle":"2022-04-01T15:12:37.69794Z","shell.execute_reply.started":"2022-04-01T15:12:34.225388Z","shell.execute_reply":"2022-04-01T15:12:37.697249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# создадим даталоадеры\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=32, \n    shuffle=True,\n    pin_memory=True, \n    drop_last=True\n)\nval_dataloader = DataLoader(\n    val_dataset, \n    batch_size=32,\n    pin_memory=True\n)","metadata":{"id":"7mePKZtWOcdW","execution":{"iopub.status.busy":"2022-04-01T15:12:37.700926Z","iopub.execute_input":"2022-04-01T15:12:37.701138Z","iopub.status.idle":"2022-04-01T15:12:37.705666Z","shell.execute_reply.started":"2022-04-01T15:12:37.701111Z","shell.execute_reply":"2022-04-01T15:12:37.704974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 2. Рекуррентная сеть для классификации аудиозаписей по сырому сигналу (2 балла)\n\nПо своей сути аудиозапись является ни чем иным, как временным рядом - замеры микрофона делаются через равные промежутки времени и хранятся в виде последовательности. \n\nКак известно, рекуррентные сети отлично подходят для работы с различными последовательностями, в том числе и с временными рядами.\n\nОбучим простую реккурентную сеть для классификации аудиозаписей.\n\n1. Разбейте аудизаписить на окошки размером `1024` с шагом `256`. Для этих целей отлично подойдет метод `torch.Tensor.unfold`.\n1. Применим к каждому получившемуся окну аудиосигнала полносвязную сеть с активациями `ReLU` и  внутренними размерностям `(1024 -> 256 -> 64 -> 16)`.\n2. По получившимся последовательностям пройдемся двунаправленой (`bidirectional=True`) LSTM с двумя слоями (`layers=2`).\n3. Склеим c помощью `torch.cat` последние `hidden_state` для каждого слоя и применим к ним полносвязную сеть `(2 * hidden_size * num_layers -> 256 -> 10)` с активацией `ReLU`.\n\n![title](https://github.com/hse-ds/iad-applied-ds/raw/91afbcd1878c8a1d1df12d617da31a761b38aed0/2022/hw/hw2/imgs/rnn_raw.png)\n\n*Совет*: для убыстрения обучения имеет смысл в полносвязные сети добавить `BatchNorm`.","metadata":{"id":"lotD7Qf7kgYD"}},{"cell_type":"markdown","source":"#### Важно\nДля задачи классификации мы обычно применяли сигмоиду для выходов последнего слоя, однако в данной домашней работе результаты получаются лучше без ее использования, поэтому осознанно откажемся от нее","metadata":{}},{"cell_type":"code","source":"class RecurrentRawAudioClassifier(nn.Module):\n    def __init__(\n        self, \n        num_classes=10,\n        window_length=1024,\n        hop_length=256,\n        hidden=256,\n        num_layers=2\n    ) -> None:\n        super().__init__()\n\n        self.window_length = window_length\n        self.hop_length = hop_length\n\n        self.first_mlp = nn.Sequential(\n            nn.Linear(window_length, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Linear(64, 16)\n        )\n\n        self.rnn = nn.LSTM(input_size=16, hidden_size=hidden, num_layers=num_layers,\n                           batch_first=True, bidirectional=True)\n\n        self.final_mlp = nn.Sequential(\n            nn.Linear(2 * hidden * num_layers, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes),\n            # nn.Sigmoid() # без сигмоиды лучше выходит, честно\n        )\n\n    def forward(self, x, lens) -> torch.Tensor:\n        # разбейте сигнал на окна \n        # batch_windows.shape == (B, NUM WINDOWS, 1024)\n        batch_windows = x.unfold(1, self.window_length,  self.hop_length)\n\n        # примените к каждому окну полносвязную сеть\n        # batch_windows_feautures.shape == (B, NUM WINDOWS, 16)\n        # из-за батчнормов что-то не выходило без цикла\n        batch_windows_feautures = torch.stack(tuple(map(self.first_mlp, batch_windows)))\n\n        # примените к получившемся последовательностям LSTM и возьмите hidden state\n        __, (hidden, __) = self.rnn(batch_windows_feautures)\n        \n        # склейте hidden_state по слоям\n        # hidden_flattened.shape = (B, 2 * hidden_size * num_layers)\n        hidden_flattened =  torch.cat([hidden[0], hidden[1], hidden[2], hidden[3]], dim=1)\n        \n        # примените полносвязную сеть и получим логиты классов (но у нас не логиты)\n        return self.final_mlp(hidden_flattened)","metadata":{"id":"oWASUw4LnY9n","execution":{"iopub.status.busy":"2022-04-01T15:12:37.706987Z","iopub.execute_input":"2022-04-01T15:12:37.707696Z","iopub.status.idle":"2022-04-01T15:12:37.720275Z","shell.execute_reply.started":"2022-04-01T15:12:37.707659Z","shell.execute_reply":"2022-04-01T15:12:37.719405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Обучим получившуюся модель.","metadata":{"id":"nhes4lBeqf8j"}},{"cell_type":"code","source":"def train_audio_clfr(\n    model, \n    optimizer, \n    train_dataloader, \n    sr,\n    criterion=torch.nn.CrossEntropyLoss(),\n    data_transform=None, \n    augmentation=None,\n    num_epochs=10, device='cuda:0',\n    verbose_num_iters=10\n):\n    model.train()\n    iter_i = 0\n\n    train_losses = []\n    train_accuracies = []\n\n    for epoch in range(num_epochs):  \n        for batch in train_dataloader:\n            x = batch[\"x\"].to(device)\n            y = batch[\"y\"].to(device)\n            lens = batch[\"len\"].to(device)\n\n            # применяем преобразование входных данных\n            if data_transform:\n                x, lens = data_transform(x, lens, device=device, sr=sr)\n\n            # примеменяем к логмелспектрограмме аугментацию\n            if augmentation:\n                x, lens = augmentation(x, lens)\n\n            probs = model(x, lens)\n            optimizer.zero_grad()\n            loss = criterion(probs, y)\n            loss.backward()\n            optimizer.step()\n\n            train_losses.append(loss.item())\n\n            # считаем точность предсказания\n            pred_cls = probs.argmax(dim=-1)\n            train_accuracies.append((pred_cls == y).float().mean().item())\n\n            iter_i += 1\n\n            # раз в verbose_num_iters визуализируем наши лоссы и семплы\n            if iter_i % verbose_num_iters == 0:\n                clear_output(wait=True)\n\n                print(f\"Epoch {epoch}\")\n\n                plt.figure(figsize=(10, 5))\n\n                plt.subplot(1, 2, 1)\n                plt.xlabel(\"Iteration\")\n                plt.ylabel(\"Train loss\")\n                plt.plot(np.arange(iter_i), train_losses)\n\n                plt.subplot(1, 2, 2)\n                plt.xlabel(\"Iteration\")\n                plt.ylabel(\"Train acc\")\n                plt.plot(np.arange(iter_i), train_accuracies)\n\n                plt.show()\n\n    model.eval()","metadata":{"id":"597FI7NglRXI","execution":{"iopub.status.busy":"2022-04-01T15:12:37.724282Z","iopub.execute_input":"2022-04-01T15:12:37.72465Z","iopub.status.idle":"2022-04-01T15:12:37.73891Z","shell.execute_reply.started":"2022-04-01T15:12:37.724615Z","shell.execute_reply":"2022-04-01T15:12:37.738066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# создадим объекты модели и оптимизатор\nrnn_raw = RecurrentRawAudioClassifier()\nrnn_raw.to(device)\noptim = torch.optim.Adam(rnn_raw.parameters(), lr=3e-4)","metadata":{"id":"taMJCqQyrB26","execution":{"iopub.status.busy":"2022-04-01T15:12:37.740171Z","iopub.execute_input":"2022-04-01T15:12:37.740655Z","iopub.status.idle":"2022-04-01T15:12:40.271641Z","shell.execute_reply.started":"2022-04-01T15:12:37.740618Z","shell.execute_reply":"2022-04-01T15:12:40.266291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обучим модель\ntrain_audio_clfr(rnn_raw, optim, train_dataloader, train_dataset.sr, num_epochs=10)","metadata":{"id":"lPffrq3BLINd","execution":{"iopub.status.busy":"2022-04-01T15:12:40.275888Z","iopub.execute_input":"2022-04-01T15:12:40.276157Z","iopub.status.idle":"2022-04-01T15:24:54.458643Z","shell.execute_reply.started":"2022-04-01T15:12:40.276118Z","shell.execute_reply":"2022-04-01T15:24:54.457057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посчитаем метрики на валидационном датасете.","metadata":{"id":"6SM4ZM68wj4g"}},{"cell_type":"code","source":"def plot_confusion_matrix(model, val_dataloader, sr, device, data_transform=None):\n    pred_true_pairs = []\n    for batch in val_dataloader:\n        x = batch[\"x\"].to(device)\n        y = batch[\"y\"].to(device)\n        lens = batch[\"len\"].to(device)\n\n        with torch.no_grad():\n            if data_transform:\n                x, lens = data_transform(x, lens, sr=sr, device=device)\n\n            probs = model(x, lens)\n\n            pred_cls = probs.argmax(dim=-1)\n\n        for pred, true in zip(pred_cls.cpu().detach().numpy(), y.cpu().numpy()):\n            pred_true_pairs.append((pred, true))\n\n    print(f\"Val accuracy: {np.mean([p[0] == p[1] for p in pred_true_pairs])}\")\n\n    cm_df = pd.DataFrame(\n        confusion_matrix(\n            [p[1] for p in pred_true_pairs], \n            [p[0] for p in pred_true_pairs], \n            normalize=\"true\"\n        ),\n        columns=classes, \n        index=classes\n    )\n    sn.heatmap(cm_df, annot=True)","metadata":{"id":"GSx9f4WwwsF6","execution":{"iopub.status.busy":"2022-04-01T15:24:54.459952Z","iopub.execute_input":"2022-04-01T15:24:54.460297Z","iopub.status.idle":"2022-04-01T15:24:54.469771Z","shell.execute_reply.started":"2022-04-01T15:24:54.460258Z","shell.execute_reply":"2022-04-01T15:24:54.46872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(rnn_raw, val_dataloader, train_dataset.sr, device)","metadata":{"id":"WFHiwGJwwyj8","outputId":"a7bafcc5-b7a3-43ed-d658-73dc89a9a16f","execution":{"iopub.status.busy":"2022-04-01T15:24:54.471329Z","iopub.execute_input":"2022-04-01T15:24:54.471581Z","iopub.status.idle":"2022-04-01T15:25:02.999574Z","shell.execute_reply.started":"2022-04-01T15:24:54.471546Z","shell.execute_reply":"2022-04-01T15:25:02.998896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Вопрос* : Сильно ли отличается качество модели на тренировочной и валидационной выборке? Если да, то как думаете, в чем причина?\n\nДа, качество сильно отличается (на трейне около 0.8 - 0.9, на тесте - около 0.2)\n\nМодель сильно переобучилась, возможно, из-за\n* не самого лучшего представления сигнала (raw), которое неустойчиво и слишком отличается для разных дорожек и модели сложно находить что-то общее даже для записей из одного класса; \n* не подобранных гиперпараметров (например, если бы мы обучали сеть меньшее число эпох, то качество на валидации могло быть выше)","metadata":{"id":"Uo_1cnkGzyNV"}},{"cell_type":"markdown","source":"## Задание 3. Построение Мел-cпектрограмм. (2 балла)\n\nСырой сигнал очень чувствителен ко многим факторам - увеличение/уменьшение громкости, внешние шумы, сменение тембра говорящего очень резко меняют сырой сигнал. Это влияет и на качество глубоких сетей, обученных на сыром аудиосигнале.\n\nДля построения надежных и устойчивых к переобучению моделей используют другое представление аудиоданных - спектрограммы, в том числе Мел-спектрограмму.\n\nИдея её построения заключается в следующем:\n1. Сигнал разбивается на временные интервалы (с пересечениями)\n2. К каждому временному интервалу применяется фильтр (как правило косинусоидальный)\n3. К профильтрованному сигналу применяется дискретное преобразование Фурье и вычисляются спектральные признаки сигнала.\n4. Спектральные признаки с помощью логарифмического преобразования приводятся в Мел-шкалу.\n\n![image](https://antkillerfarm.github.io/images/img2/Spectrogram_5.png)\n\nВ этом задании мы сами шаг за шагом напишем алгоритм построения мелспектрограммы и сравнимся с референсной функцией из `torchaudio`.","metadata":{"id":"5G8-l8yuRNLL"}},{"cell_type":"code","source":"from torchaudio.transforms import MelSpectrogram\n\n# референсная функця\ndef compute_log_melspectrogram_reference(\n    wav_batch, \n    lens,\n    sr,\n    device=\"cpu\"\n):\n    featurizer = MelSpectrogram(\n        sample_rate=sr,\n        n_fft=1024,\n        win_length=1024,\n        hop_length=256,\n        n_mels=64,\n        center=False,\n        ).to(device)\n\n    return torch.log(featurizer(wav_batch).clamp(1e-5)), lens // 256","metadata":{"id":"wubdQiRcGdOV","execution":{"iopub.status.busy":"2022-04-01T15:25:03.000824Z","iopub.execute_input":"2022-04-01T15:25:03.001169Z","iopub.status.idle":"2022-04-01T15:25:03.007747Z","shell.execute_reply.started":"2022-04-01T15:25:03.00113Z","shell.execute_reply":"2022-04-01T15:25:03.00705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# возьмем случайный батч\nfor batch in train_dataloader:\n    break\n\nwav_batch = batch[\"x\"]\nlens = batch[\"len\"]\n\n# посчитаем лог мелспектрограммы\nlog_melspect, lens = compute_log_melspectrogram_reference(wav_batch, lens, train_dataset.sr)\n\n# нарисуем получившиеся референсные значения\nfig, axes = plt.subplots(5, figsize=(16, 8))\n\nfor i in range(5):\n    axes[i].axis(\"off\")\n    axes[i].set_title(f\"Reference log melspectorgram {i}\")\n    axes[i].imshow(log_melspect[i].numpy())","metadata":{"id":"_GyXa09v2tAH","outputId":"a15fdcce-8cd5-4fec-b80b-5ff3effcecab","execution":{"iopub.status.busy":"2022-04-01T15:25:03.008931Z","iopub.execute_input":"2022-04-01T15:25:03.009666Z","iopub.status.idle":"2022-04-01T15:25:04.474701Z","shell.execute_reply.started":"2022-04-01T15:25:03.009628Z","shell.execute_reply":"2022-04-01T15:25:04.474073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь сделаем то же самое сами. ","metadata":{"id":"y9NHPcQ9IL7-"}},{"cell_type":"code","source":"sr = train_dataset.sr\nn_fft=1024\nwin_length=1024\nhop_length=256\nn_mels=64","metadata":{"id":"SQackPL0JAlI","execution":{"iopub.status.busy":"2022-04-01T15:25:04.47599Z","iopub.execute_input":"2022-04-01T15:25:04.47676Z","iopub.status.idle":"2022-04-01T15:25:04.481145Z","shell.execute_reply.started":"2022-04-01T15:25:04.476722Z","shell.execute_reply":"2022-04-01T15:25:04.480468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nДля начала с помощью метода `unfold` разделим аудиосигнал на окна размера `win_lenght` через промежутки `hop_lenght`.","metadata":{"id":"S3QjWmr0JK6g"}},{"cell_type":"code","source":"windows = wav_batch.unfold(1, win_length, hop_length)\nassert windows.shape == (32, 1497, 1024)","metadata":{"id":"T2PhQ8MWQQOe","execution":{"iopub.status.busy":"2022-04-01T15:25:04.482431Z","iopub.execute_input":"2022-04-01T15:25:04.482832Z","iopub.status.idle":"2022-04-01T15:25:04.491739Z","shell.execute_reply.started":"2022-04-01T15:25:04.482795Z","shell.execute_reply":"2022-04-01T15:25:04.491022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нарисуем и проиграем сигнал из одного окна.","metadata":{"id":"VgDm4pdsJreZ"}},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.plot(windows[0, 0])\n\ndisplay.Audio(windows[0, 0], rate=train_dataset.sr)","metadata":{"id":"rUF4iFkbQsqt","outputId":"2c0b7230-63c0-41da-e5ef-7e3803e34030","execution":{"iopub.status.busy":"2022-04-01T15:25:04.492994Z","iopub.execute_input":"2022-04-01T15:25:04.493264Z","iopub.status.idle":"2022-04-01T15:25:04.716715Z","shell.execute_reply.started":"2022-04-01T15:25:04.493229Z","shell.execute_reply":"2022-04-01T15:25:04.716065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь нам надо применить косинуисальный фильтр к сигналу из окна. Для этого с помощью `torch.hann_window` создадим косинусоидальный фильтр и умножим его поэлементно на все окна.","metadata":{"id":"A3rp6KtjKWmG"}},{"cell_type":"code","source":"filter = torch.hann_window(win_length)\nwindows_with_applied_filter = windows * filter[None, None, :]","metadata":{"id":"jTtg612XQ8NB","execution":{"iopub.status.busy":"2022-04-01T15:25:04.718064Z","iopub.execute_input":"2022-04-01T15:25:04.71858Z","iopub.status.idle":"2022-04-01T15:25:04.82547Z","shell.execute_reply.started":"2022-04-01T15:25:04.718543Z","shell.execute_reply":"2022-04-01T15:25:04.824684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.plot(windows_with_applied_filter[0, 0])\n\ndisplay.Audio(windows_with_applied_filter[0, 0], rate=train_dataset.sr)","metadata":{"id":"UJnHHDEDo6f7","outputId":"641c4041-bc20-4668-ebbd-45cf7838b761","execution":{"iopub.status.busy":"2022-04-01T15:25:04.826902Z","iopub.execute_input":"2022-04-01T15:25:04.827178Z","iopub.status.idle":"2022-04-01T15:25:05.045566Z","shell.execute_reply.started":"2022-04-01T15:25:04.82714Z","shell.execute_reply":"2022-04-01T15:25:05.044889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"С помощью `torch.fft.fft` примените дискретное преобразование фурье к каждому окну и возьмите первые `n_fft // 2 + 1` компоненты.\n\nДальше с помощью возведения элементов тензора в квадрат и `torch.abs()` получите магнитуды.","metadata":{"id":"191JHzI_K0KC"}},{"cell_type":"code","source":"fft_features = torch.fft.fft(windows_with_applied_filter)[:,:,:(n_fft // 2 + 1)] \nfft_magnitudes = torch.abs(fft_features ** 2)\nassert fft_magnitudes.shape == (32, 1497, 513)","metadata":{"id":"TfwpkUZcOflD","execution":{"iopub.status.busy":"2022-04-01T15:25:05.046761Z","iopub.execute_input":"2022-04-01T15:25:05.047016Z","iopub.status.idle":"2022-04-01T15:25:05.692767Z","shell.execute_reply.started":"2022-04-01T15:25:05.046978Z","shell.execute_reply":"2022-04-01T15:25:05.692028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Через `torchaudio.transforms.MelScale` создайте класс для перевода магнитуд в Мел-шкалу.","metadata":{"id":"pIvEu-3aMdS2"}},{"cell_type":"code","source":"from torchaudio.transforms import MelScale\n\nmelscale = MelScale(sample_rate=sr, n_mels=n_mels, n_stft=n_fft // 2 + 1)","metadata":{"id":"PsHfVe4zOoYl","execution":{"iopub.status.busy":"2022-04-01T15:25:05.694153Z","iopub.execute_input":"2022-04-01T15:25:05.69458Z","iopub.status.idle":"2022-04-01T15:25:05.700115Z","shell.execute_reply.started":"2022-04-01T15:25:05.694542Z","shell.execute_reply":"2022-04-01T15:25:05.699415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нелинейное преобразование для перевода в Мел-шкалу выглядит следующим образом.","metadata":{"id":"tNUZz4m2NcIH"}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.axis(\"off\")\nplt.imshow(melscale.fb.numpy().transpose());","metadata":{"id":"T2nirK_MR9PM","outputId":"dccb23c8-77a7-42e8-a8b6-104327d848a2","execution":{"iopub.status.busy":"2022-04-01T15:25:05.701445Z","iopub.execute_input":"2022-04-01T15:25:05.701716Z","iopub.status.idle":"2022-04-01T15:25:05.783641Z","shell.execute_reply.started":"2022-04-01T15:25:05.701679Z","shell.execute_reply":"2022-04-01T15:25:05.782907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Примените Мел-шкалу к магнитудам.","metadata":{"id":"7YVcdanFNnqx"}},{"cell_type":"code","source":"mel_spectrogram = melscale(fft_magnitudes.transpose(-1, -2))\nassert mel_spectrogram.shape == (32, 64, 1497)","metadata":{"id":"SRo-H_r2SVA_","execution":{"iopub.status.busy":"2022-04-01T15:25:05.784733Z","iopub.execute_input":"2022-04-01T15:25:05.785094Z","iopub.status.idle":"2022-04-01T15:25:05.845247Z","shell.execute_reply.started":"2022-04-01T15:25:05.785061Z","shell.execute_reply":"2022-04-01T15:25:05.844432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сделайте обрезку значений по `1e-5` и примените `torch.log` для получения логарифмированной Мел-спектрограммы.","metadata":{"id":"jg9xL0GSORGw"}},{"cell_type":"code","source":"logmel_spectrogram = torch.log(mel_spectrogram.clamp(1e-5)) \nassert logmel_spectrogram.shape == (32, 64, 1497)","metadata":{"id":"5jTzCF3qSp1d","execution":{"iopub.status.busy":"2022-04-01T15:25:05.846409Z","iopub.execute_input":"2022-04-01T15:25:05.846787Z","iopub.status.idle":"2022-04-01T15:25:05.858447Z","shell.execute_reply.started":"2022-04-01T15:25:05.846752Z","shell.execute_reply":"2022-04-01T15:25:05.857712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Полученные логарифмированные Мел-Спектрограммы должны совпадать с референсными.","metadata":{"id":"Qpf_PuvSOzjK"}},{"cell_type":"code","source":"# нарисуем получившиеся значения\nfig, axes = plt.subplots(5, figsize=(16, 8))\n\nfor i in range(5):\n    axes[i].axis(\"off\")\n    axes[i].set_title(f\"Your log melspectorgram {i}\")\n    axes[i].imshow(logmel_spectrogram[i].numpy())","metadata":{"id":"lcdq-X8sSsd2","outputId":"a2095637-5fcc-491f-e0f3-8fdd456048ec","execution":{"iopub.status.busy":"2022-04-01T15:25:05.859513Z","iopub.execute_input":"2022-04-01T15:25:05.859945Z","iopub.status.idle":"2022-04-01T15:25:06.254669Z","shell.execute_reply.started":"2022-04-01T15:25:05.8599Z","shell.execute_reply":"2022-04-01T15:25:06.253993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь оформим эту логику в функцию.","metadata":{"id":"jY7egzCCPcw5"}},{"cell_type":"code","source":"# ваша реализация\ndef compute_log_melspectrogram(\n    wav_batch,\n    lens,\n    sr,\n    device=\"cpu\"\n):\n    \n    windows = wav_batch.unfold(1, win_length, hop_length).to(device)\n\n    filter = torch.hann_window(win_length).to(device) \n    windows_with_applied_filter = windows * filter[None, None, :]\n\n    fft_features = torch.fft.fft(windows_with_applied_filter)[:,:,:(n_fft // 2 + 1)] \n    fft_magnitudes = torch.abs(fft_features ** 2)\n\n    melscale = MelScale(sample_rate=sr, n_mels=n_mels, n_stft=n_fft // 2 + 1).to(device)\n    mel_spectrogram = melscale(fft_magnitudes.transpose(-1, -2)).to(device)\n\n    return torch.log(mel_spectrogram.clamp(1e-5)).to(device), lens // 256 # как в шаблоне\n","metadata":{"id":"re2zffcEPb2F","execution":{"iopub.status.busy":"2022-04-01T15:25:06.257373Z","iopub.execute_input":"2022-04-01T15:25:06.257569Z","iopub.status.idle":"2022-04-01T15:25:06.266598Z","shell.execute_reply.started":"2022-04-01T15:25:06.257543Z","shell.execute_reply":"2022-04-01T15:25:06.265886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Финальная проверка.","metadata":{"id":"cPCTsOiLRRPs"}},{"cell_type":"code","source":"assert torch.allclose(\n    compute_log_melspectrogram_reference(wav_batch, lens, train_dataset.sr)[0],\n    compute_log_melspectrogram(wav_batch, lens, train_dataset.sr)[0],\n    atol=1e-5\n)","metadata":{"id":"NZZj8q_ZQuHy","execution":{"iopub.status.busy":"2022-04-01T15:25:06.268266Z","iopub.execute_input":"2022-04-01T15:25:06.268491Z","iopub.status.idle":"2022-04-01T15:25:07.918717Z","shell.execute_reply.started":"2022-04-01T15:25:06.268465Z","shell.execute_reply":"2022-04-01T15:25:07.917945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 4. Рекуррентная сеть для классификации аудиозаписей по логарифмированным Мел-спектрограммам (1 балл)\n\nИзмените реализацию рекуррентной сети из задания 2, таким образом, чтобы она вместо сырого сигнала смогла принимать логарифмированные Мел-спетрограммы:\n1. Уберите шаги 1-2\n2. Сделайте вход LSTM равным 64\n\n![arch_mel](https://github.com/hse-ds/iad-applied-ds/raw/91afbcd1878c8a1d1df12d617da31a761b38aed0/2022/hw/hw2/imgs/rnn_mel.png)\n\n**За реализацию архитектуры дается 0.5 балла.**","metadata":{"id":"NN5KS-uufYDA"}},{"cell_type":"code","source":"class RecurrentMelSpectClassifier(nn.Module):\n    def __init__(\n        self, \n        num_classes=10,\n        window_length=1024,\n        hop_length=256,\n        hidden=256,\n        num_layers=2\n    ) -> None:\n        super().__init__()\n\n        self.window_length = window_length\n        self.hop_length = hop_length\n\n        self.rnn = nn.LSTM(input_size=64, hidden_size=hidden, num_layers=num_layers, \n                            bidirectional=True)\n\n        self.final_mlp = nn.Sequential(\n            nn.Linear(2 * hidden * num_layers, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes),\n            # nn.Sigmoid() # опять лучше без нее\n        )\n\n    def forward(self, x, lens):\n        \n        __, (hidden, __) = self.rnn(x.permute(2, 0, 1))\n        hidden_flattened = torch.cat([hidden[0], hidden[1], hidden[2], hidden[3]], dim=1)\n        return self.final_mlp(hidden_flattened)\n\n","metadata":{"id":"lrlBRTF-fV86","execution":{"iopub.status.busy":"2022-04-01T15:25:07.919949Z","iopub.execute_input":"2022-04-01T15:25:07.920767Z","iopub.status.idle":"2022-04-01T15:25:07.929724Z","shell.execute_reply.started":"2022-04-01T15:25:07.920724Z","shell.execute_reply":"2022-04-01T15:25:07.928899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_mel = RecurrentMelSpectClassifier()\nrnn_mel.to(device)\n\noptim = torch.optim.Adam(rnn_mel.parameters(), lr=9e-5)","metadata":{"id":"a9Z4a-W6jBQX","execution":{"iopub.status.busy":"2022-04-01T15:25:07.931194Z","iopub.execute_input":"2022-04-01T15:25:07.931619Z","iopub.status.idle":"2022-04-01T15:25:07.964333Z","shell.execute_reply.started":"2022-04-01T15:25:07.931582Z","shell.execute_reply":"2022-04-01T15:25:07.963671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_audio_clfr(rnn_mel, optim, train_dataloader, train_dataset.sr, \n                 data_transform=compute_log_melspectrogram, num_epochs=15)","metadata":{"id":"at77cRtEjt-r","outputId":"933bec9b-e602-47e2-bc35-2e73bccca1eb","execution":{"iopub.status.busy":"2022-04-01T15:25:07.96548Z","iopub.execute_input":"2022-04-01T15:25:07.965712Z","iopub.status.idle":"2022-04-01T15:42:09.482316Z","shell.execute_reply.started":"2022-04-01T15:25:07.96568Z","shell.execute_reply":"2022-04-01T15:42:09.481658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посчитаем метрики на валидационном датасете.","metadata":{"id":"dpl1QhKlVkot"}},{"cell_type":"markdown","source":"**Задание: для получения 0.5 балла сделайте подбор гиперпараметров и добейтесь accuracy модели выше 0.8 на валидационном датасете.**","metadata":{"id":"hOXx3pG58cHm"}},{"cell_type":"code","source":"plot_confusion_matrix(rnn_mel, val_dataloader, train_dataset.sr, device, \n                      data_transform=compute_log_melspectrogram)","metadata":{"id":"U2WPjo4tjyy0","outputId":"7333d474-fb79-4013-a3cc-7e23e022e0db","execution":{"iopub.status.busy":"2022-04-01T15:42:09.483755Z","iopub.execute_input":"2022-04-01T15:42:09.484024Z","iopub.status.idle":"2022-04-01T15:42:17.473621Z","shell.execute_reply.started":"2022-04-01T15:42:09.483988Z","shell.execute_reply":"2022-04-01T15:42:17.47226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 5. Cверточная сеть для классификации аудиозаписей по мелспектрограммам. (2 балла)\n\nЛегко заметить, что мелспектрограммы имеют четко выраженные паттерны - если приноровиться, то даже человек, посмотрев на мелспектрограмму, сможет _визуально_ проклассифицировать объект.\n\nЭто позволяет свести задачу классификации аудиозаписей к задаче классификации картинок.\n\nРеализуем такую сверточную сеть:\n\n* 2x (Conv2d 3x3 @ 16, BatchNorm2d, ReLU)\n* MaxPoll 2x2\n* 2x (Conv2d 3x3 @ 32, BatchNorm2d, ReLU)\n* MaxPoll 2x2\n* 2x (Conv2d 3x3 @ 64, BatchNorm2d, ReLU)\n* MaxPoll 2x2\n* (Conv2d 3x3 @ 128, BatchNorm2d, ReLU)\n* (Conv2d 2x2 @ 128, BatchNorm2d, ReLU)\n* Global MaxPoll\n* Fully Connected 128, ReLU\n* Fully Connected 10\n\nСовет: подобная архитектура была реализована в [**PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition**](https://arxiv.org/pdf/1912.10211.pdf), можете использовать эту статью как референс.\n\n\n**За реализацию архитектуры дается 1.5 балла.**","metadata":{"id":"Od6HOpq8FXgH"}},{"cell_type":"code","source":"from torch.nn.modules.batchnorm import BatchNorm2d\nclass CNN10(nn.Module):\n    def __init__(self, num_classes=10, hidden=16):\n        super().__init__()\n\n        self.cnn_backbone = nn.Sequential(\n            nn.Conv2d(1, 16, 3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, 3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(16, 32, 3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, 3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, 3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, 3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, 3),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, 2),\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n        )\n\n        self.final_mlp = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes),\n            # nn.Sigmoid()\n        )\n\n    def forward(self, x, lens):\n        z = self.cnn_backbone(x[:, None, :, :])\n        z = torch.nn.functional.max_pool2d(z, kernel_size=z.size()[2:])[:, :, 0, 0]\n        return self.final_mlp(z)","metadata":{"id":"3BSb2SWxFXgI","execution":{"iopub.status.busy":"2022-04-01T15:42:17.47514Z","iopub.execute_input":"2022-04-01T15:42:17.475619Z","iopub.status.idle":"2022-04-01T15:42:17.487593Z","shell.execute_reply.started":"2022-04-01T15:42:17.47558Z","shell.execute_reply":"2022-04-01T15:42:17.486874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = CNN10()\ncnn.to(device);\n\noptim = torch.optim.Adam(cnn.parameters(), lr=3e-4)","metadata":{"id":"9mmkvk5nFXgK","execution":{"iopub.status.busy":"2022-04-01T15:42:17.488687Z","iopub.execute_input":"2022-04-01T15:42:17.48917Z","iopub.status.idle":"2022-04-01T15:42:17.508665Z","shell.execute_reply.started":"2022-04-01T15:42:17.489127Z","shell.execute_reply":"2022-04-01T15:42:17.508017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_audio_clfr(cnn, optim, train_dataloader, train_dataset.sr, \n                 data_transform=compute_log_melspectrogram_reference,\n                 num_epochs=20)","metadata":{"id":"_-z39AbO8_3Z","outputId":"3c868f9c-d696-4ede-d144-f581186c11e3","execution":{"iopub.status.busy":"2022-04-01T15:42:17.509867Z","iopub.execute_input":"2022-04-01T15:42:17.510263Z","iopub.status.idle":"2022-04-01T15:52:28.659021Z","shell.execute_reply.started":"2022-04-01T15:42:17.510205Z","shell.execute_reply":"2022-04-01T15:52:28.658357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Задание: для получения 0.5 балла сделайте подбор гиперпараметров и добейтесь accuracy модели выше 0.85 на валидационном датасете.**","metadata":{"id":"lM-B_VQZ-dJX"}},{"cell_type":"code","source":"plot_confusion_matrix(cnn, val_dataloader, train_dataset.sr, device, \n                      data_transform=compute_log_melspectrogram)","metadata":{"id":"PVbI7IAcQNjX","execution":{"iopub.status.busy":"2022-04-01T15:52:28.664394Z","iopub.execute_input":"2022-04-01T15:52:28.664914Z","iopub.status.idle":"2022-04-01T15:52:33.060749Z","shell.execute_reply.started":"2022-04-01T15:52:28.664861Z","shell.execute_reply":"2022-04-01T15:52:33.060083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Задание 6. Аугментация SpecAugment (2 балла)\n\nОбычно датасеты с аудиозаписями довольно малы. Наш датасет тому пример - всего 4500 объектов в обучающей выборке. Обучение глубокий сетей с большим кол-вом параметров на таких датасетах часто ведет к переобучению и проседанию метрик на валидационном и тестовом датасетах.\n\nДля борьбы с переобучением имеет смысл использовать аугментацию данных. Для мелспектрограмм была придумана аугментация под названием SpecAugment.\n\nСмысл её очень прост - зануление спектрограммы по временным промежуткам и по мел-частотам:\n1. Выбираются несколько временных промежутков ${[t^1_i, t^2_i]}$ и заполняют спектрограмму $s[t^1_i : t^2_i, :]$ значением $v$.\n\n2. Выбираются несколько промежутков мелчастот ${[m^1_i, m^2_i]}$ и заполняют спектрограмму $s[:, m^1_i : m^2_i]$ значением $v$.\n\nВ качестве значения $v$ выбирают:\n1. `'mean'`: среднее по спектрограмме\n2. `'min'`: минимум по спектрограмме\n3. `'max'`: максимум по спектрограмме\n5. `v`: некоторая константа\n\nСовет: описание аугментации можно найти здесь: [link](https://neurohive.io/ru/novosti/specaugment-novyj-metod-augmentacii-audiodannyh-ot-google-ai/), можете использовать эту ссылку как референс.\n\n![specaugment](https://neurohive.io/wp-content/uploads/2019/04/image6.png)\n\nВ этом задании Вам предлагается реализовать аугментацию SpecAugment.\n\n**За реализацию аугментации дается 1.5 балла.**\n","metadata":{"id":"cu9fpMOikY6L"}},{"cell_type":"markdown","source":"Так как в задании нет особых пояснений, то я решил делать так: аугментировать лишь ту часть (по временным промежуткам), которую мы не паддили ранее. Аугментировать искусственно созданную пустоту мне кажется, менее правильным. Также для каждого элемента батча я буду делать новую аугментацию (а не использовать одну и ту же для всего батча), это добавит больше рандома. \n\nНа всякий случай приведу и реализацию аугментации по всей дорожке (одну для всего батча), но ее использовать не будем:","metadata":{}},{"cell_type":"code","source":"import random\n\nclass SpectAugmentDefault:\n    def __init__(\n        self,\n        filling_value = \"mean\",\n        n_freq_masks = 2,\n        n_time_masks = 2,\n        max_freq = 10,\n        max_time = 50,\n    ):\n\n        self.filling_value = filling_value\n        self.n_freq_masks = n_freq_masks\n        self.n_time_masks = n_time_masks\n        self.max_freq = max_freq\n        self.max_time = max_time\n\n    def __call__(self, spect, lens):\n        ### YOUR CODE IS HERE ######\n        spect1 = spect.clone().detach()\n        \n        if self.filling_value == 'mean':\n            to_fill = torch.mean(spect1)\n        elif self.filling_value == 'min':\n            to_fill = torch.min(spect1)\n        elif self.filling_value == 'max':\n            to_fill = torch.max(spect1)\n        else:\n            to_fill = self.filling_value\n        \n        max_f = spect1.shape[1]\n        max_t = spect1.shape[2]\n\n        for i in range(self.n_freq_masks):\n            interval_len = random.randint(1, self.max_freq)\n            start_point = max([random.randint(0, max_f) - interval_len, 0])\n            spect1[:, start_point:(start_point + interval_len),:] = to_fill\n            \n        for i in range(self.n_time_masks):\n            interval_len = random.randint(1, self.max_time)\n            start_point = max([random.randint(0, max_t) - interval_len, 0])\n            spect1[:, :, start_point:(start_point + interval_len)] = to_fill\n        \n        return spect1, lens\n        ### THE END OF YOUR CODE ###","metadata":{"execution":{"iopub.status.busy":"2022-04-01T15:52:33.062031Z","iopub.execute_input":"2022-04-01T15:52:33.062457Z","iopub.status.idle":"2022-04-01T15:52:33.073845Z","shell.execute_reply.started":"2022-04-01T15:52:33.062417Z","shell.execute_reply":"2022-04-01T15:52:33.073037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# применим аугментацию к данным\nfor batch in train_dataloader:\n    break\n\nx = batch[\"x\"].to(device)\nlens = batch[\"len\"].to(device)\nx_logmel, lens = compute_log_melspectrogram_reference(x, lens, sr=train_dataset.sr, device=device)\nx_logmel_augmented, lens = SpectAugmentDefault()(x_logmel, lens)\n\n# нарисуем спектрограмму до и после аугментации\nplt.figure(figsize=(20, 5))\nplt.subplot(2, 1, 1)\nplt.title(\"Original log MelSpectrogram\")\nplt.axis(\"off\")\nplt.imshow(x_logmel[0].cpu().numpy())\n\nplt.subplot(2, 1, 2)\nplt.title(\"Augmented log MelSpectrogram\")\nplt.axis(\"off\")\nplt.imshow(x_logmel_augmented[0].cpu().numpy())\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-01T15:52:33.075111Z","iopub.execute_input":"2022-04-01T15:52:33.07555Z","iopub.status.idle":"2022-04-01T15:52:33.358286Z","shell.execute_reply.started":"2022-04-01T15:52:33.075512Z","shell.execute_reply":"2022-04-01T15:52:33.357606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь аугментация, которую мы будем использовать:","metadata":{}},{"cell_type":"code","source":"class SpectAugment:\n    def __init__(\n        self,\n        filling_value = \"mean\",\n        n_freq_masks = 2,\n        n_time_masks = 2,\n        max_freq = 10,\n        max_time = 50,\n    ):\n\n        self.filling_value = filling_value\n        self.n_freq_masks = n_freq_masks\n        self.n_time_masks = n_time_masks\n        self.max_freq = max_freq\n        self.max_time = max_time\n\n    def __call__(self, spect, lens):\n        ### YOUR CODE IS HERE ######\n        spect1 = spect.clone().detach()\n        \n        if self.filling_value == 'mean':\n            to_fill = torch.mean(spect1)\n        elif self.filling_value == 'min':\n            to_fill = torch.min(spect1)\n        elif self.filling_value == 'max':\n            to_fill = torch.max(spect1)\n        else:\n            to_fill = self.filling_value\n        \n        max_f = spect1.shape[1]\n        # для каждого элемента батча будет своя аугментация\n        for j in range(len(spect)):\n            # будем аугментировать лишь часть без паддинга (не зря же мы тащили длины)\n            max_t = lens[j]\n            # заполняем частоты\n            for i in range(self.n_freq_masks):\n                interval_len = random.randint(1, self.max_freq)\n                start_point = max([random.randint(0, max_f) - interval_len, 0])\n                spect1[j][start_point:(start_point + interval_len),:] = to_fill\n            # заполняем время\n            for i in range(self.n_time_masks):\n                interval_len = random.randint(1, self.max_time)\n                start_point = max([random.randint(0, max_t) - interval_len, 0])\n                spect1[j][:, start_point:(start_point + interval_len)] = to_fill\n        \n        return spect1, lens\n        ### THE END OF YOUR CODE ###","metadata":{"id":"lmfkTTrfFXgp","execution":{"iopub.status.busy":"2022-04-01T15:52:33.359739Z","iopub.execute_input":"2022-04-01T15:52:33.360199Z","iopub.status.idle":"2022-04-01T15:52:33.373037Z","shell.execute_reply.started":"2022-04-01T15:52:33.360152Z","shell.execute_reply":"2022-04-01T15:52:33.372202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# применим аугментацию к данным\nfor batch in train_dataloader:\n    break\n\nx = batch[\"x\"].to(device)\nlens = batch[\"len\"].to(device)\nx_logmel, lens = compute_log_melspectrogram_reference(x, lens, sr=train_dataset.sr, device=device)\nx_logmel_augmented, lens = SpectAugment()(x_logmel, lens)\n\n# нарисуем спектрограмму до и после аугментации\nplt.figure(figsize=(20, 5))\nplt.subplot(2, 1, 1)\nplt.title(\"Original log MelSpectrogram\")\nplt.axis(\"off\")\nplt.imshow(x_logmel[0].cpu().numpy())\n\nplt.subplot(2, 1, 2)\nplt.title(\"Augmented log MelSpectrogram\")\nplt.axis(\"off\")\nplt.imshow(x_logmel_augmented[0].cpu().numpy())\n\nplt.show()","metadata":{"id":"GXaJi2nfqE4J","outputId":"ce1d6da7-bfb8-4caa-9697-dc5f0332a679","execution":{"iopub.status.busy":"2022-04-01T16:04:04.73369Z","iopub.execute_input":"2022-04-01T16:04:04.733964Z","iopub.status.idle":"2022-04-01T16:04:05.050932Z","shell.execute_reply.started":"2022-04-01T16:04:04.733926Z","shell.execute_reply":"2022-04-01T16:04:05.050248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn = CNN10()\ncnn.to(device);\n\noptim = torch.optim.Adam(cnn.parameters(), lr=3e-4)","metadata":{"id":"XdohpTw5l6Lk","execution":{"iopub.status.busy":"2022-04-01T15:52:33.701879Z","iopub.execute_input":"2022-04-01T15:52:33.702442Z","iopub.status.idle":"2022-04-01T15:52:33.71841Z","shell.execute_reply.started":"2022-04-01T15:52:33.702402Z","shell.execute_reply":"2022-04-01T15:52:33.717725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обучим модель на данных с аугментациями\ntrain_audio_clfr(cnn, optim, train_dataloader, train_dataset.sr, \n                 data_transform=compute_log_melspectrogram,\n                 augmentation=SpectAugment(n_freq_masks=1, n_time_masks=1, filling_value='min'),\n                 num_epochs=20)","metadata":{"id":"59YvirCHm_ye","execution":{"iopub.status.busy":"2022-04-01T15:52:33.719879Z","iopub.execute_input":"2022-04-01T15:52:33.72017Z","iopub.status.idle":"2022-04-01T16:03:03.642677Z","shell.execute_reply.started":"2022-04-01T15:52:33.720131Z","shell.execute_reply":"2022-04-01T16:03:03.642015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Задание: для получения 0.5 балла сделайте подбор параметров аугментации и добейтесь accuracy модели выше 0.9 на валидационном датасете.**","metadata":{"id":"iI_0no5Z_OZv"}},{"cell_type":"code","source":"plot_confusion_matrix(cnn, val_dataloader, train_dataset.sr, device, \n                      data_transform=compute_log_melspectrogram)","metadata":{"id":"F7kgOAn-nHSU","execution":{"iopub.status.busy":"2022-04-01T16:03:03.644019Z","iopub.execute_input":"2022-04-01T16:03:03.645367Z","iopub.status.idle":"2022-04-01T16:03:08.296651Z","shell.execute_reply.started":"2022-04-01T16:03:03.645324Z","shell.execute_reply":"2022-04-01T16:03:08.295529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Получилось немного хуже, чем без аугментации, что можно объяснить рандомом. Так или иначе, нужный порог пробит. Заметим, что плохо качество на валидации выдается на, скорее, коротких звуках: выстрел пистолета, лай... В этом минус использованной нами аугментации: если записи короткие, то большая часть информативных отрезков будет зашумлена. Посмотрим ради интереса на результаты с альтернативной аугментацией (дефолтной):","metadata":{}},{"cell_type":"code","source":"cnn = CNN10()\ncnn.to(device);\n\noptim = torch.optim.Adam(cnn.parameters(), lr=3e-4)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:24:49.658879Z","iopub.execute_input":"2022-04-01T16:24:49.659148Z","iopub.status.idle":"2022-04-01T16:24:49.674316Z","shell.execute_reply.started":"2022-04-01T16:24:49.659118Z","shell.execute_reply":"2022-04-01T16:24:49.673541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обучим модель на данных с аугментациями\ntrain_audio_clfr(cnn, optim, train_dataloader, train_dataset.sr, \n                 data_transform=compute_log_melspectrogram,\n                 augmentation=SpectAugmentDefault(n_freq_masks=1, n_time_masks=1, filling_value='min'),\n                 num_epochs=20)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:24:50.369578Z","iopub.execute_input":"2022-04-01T16:24:50.369819Z","iopub.status.idle":"2022-04-01T16:35:08.798303Z","shell.execute_reply.started":"2022-04-01T16:24:50.369791Z","shell.execute_reply":"2022-04-01T16:35:08.797264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(cnn, val_dataloader, train_dataset.sr, device, \n                      data_transform=compute_log_melspectrogram)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T16:35:08.800111Z","iopub.execute_input":"2022-04-01T16:35:08.800474Z","iopub.status.idle":"2022-04-01T16:35:13.945859Z","shell.execute_reply.started":"2022-04-01T16:35:08.800435Z","shell.execute_reply":"2022-04-01T16:35:13.945079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вышло получше, но это опять же может быть рандомом, короткие звуки теперь классифицируются круче","metadata":{}},{"cell_type":"markdown","source":"Таков путь.","metadata":{}}]}